# -*- coding: utf-8 -*-
"""main1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wnjZmPnTEp_5rXzMHCIpBW9Vux5c8Tbb
"""

#!/usr/bin/env python



# imports
import cv2
import numpy as np
import time
import argparse

# own modules
import utills, plot1

confid = 0.5
thresh = 0.5
mouse_pts = []                                                   

#this is used to fetch the points marked by user on image
def get_mouse_points(event, x, y, flags, param):

    global mouse_pts
    if event == cv2.EVENT_LBUTTONDOWN:
        if len(mouse_pts) < 4:
            cv2.circle(image, (x, y), 5, (0, 0, 255), 10) #circle will appear at which user mark
        else:
            cv2.circle(image, (x, y), 5, (255, 0, 0), 10)
            
        if len(mouse_pts) >= 1 and len(mouse_pts) <= 3: #check piinted marks should be 4 
	    #print(len(mouse_pts))
            cv2.line(image, (x, y), (mouse_pts[len(mouse_pts)-1][0], mouse_pts[len(mouse_pts)-1][1]), (70, 70, 70), 2)
            if len(mouse_pts) == 3:
		#print(len(mouse_pts))
                cv2.line(image, (x, y), (mouse_pts[0][0], mouse_pts[0][1]), (70, 70, 70), 2)
        
        if "mouse_pts" not in globals():
            mouse_pts = []
        mouse_pts.append((x, y))
	print(x,y)
        #print("Point detected")
        #print(mouse_pts)
        


def calculate_social_distancing(vid_path, net, output_dir, output_vid, ln1):
    
    count = 0
    a=0
    b=0
    vs = cv2.VideoCapture(vid_path)    

    # detcect ftp video height, width
    width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(vs.get(cv2.CAP_PROP_FPS))
    #print(width,height,fps)
    
    # for birds eye view scale is set
    # Bird's eye view will only show ROI
    scale_w, scale_h = utills.get_scale(width, height)

    fourcc = cv2.VideoWriter_fourcc(*"MJPG")
    
    output_movie = cv2.VideoWriter(output_vid+ "distance.avi", fourcc, fps, (width, height))
    bird_movie = cv2.VideoWriter(output_vid+ "birdeyeview.avi", fourcc, fps, (width, height))
        
    points = []
    got=[]
    global image
    
    while True:

        (grabbed, frame) = vs.read()

        if not grabbed:
            print('here')
            break
            
        (H, W) = frame.shape[:2]
        
        # to draw ROI and vertical and horizontal 180 cm distance
        if count == 0:
            while True:
                image = frame
                cv2.imshow("image", image)
		#plt.show()
                cv2.waitKey(1)
                if len(mouse_pts) == 8:
		    #print(len(mouse_pts))
                    cv2.destroyWindow("image")
                    break
               
            points = mouse_pts      
                 
	dst = np.float32([[0, H], [W, H], [W, 0], [0, 0]])
        src = np.float32(np.array(points[:4]))
	#print(dst,src)
        prespective_transform = cv2.getPerspectiveTransform(src, dst)

        pts = np.float32(np.array([points[4:7]]))
        warped_pt = cv2.perspectiveTransform(pts, prespective_transform)[0]
       
	distance_h = np.sqrt((warped_pt[0][0] - warped_pt[2][0]) ** 2 + (warped_pt[0][1] - warped_pt[2][1]) ** 2)
        distance_w = np.sqrt((warped_pt[0][0] - warped_pt[1][0]) ** 2 + (warped_pt[0][1] - warped_pt[1][1]) ** 2)
        pnts = np.array(points[:4], np.int32)
        cv2.polylines(frame, [pnts], True, (70, 70, 70), thickness=2)

    
        
        def detect_people(frame, net, ln, personIdx=0):
            MIN_CONF = 0.3
            NMS_THRESH = 0.3
            (H, W) = frame.shape[:2]
            results = []
	    (H, W) = frame.shape[:2] #taking part of frame
	    getblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),
		swapRB=True, crop=False)
	    net.setInput(getblob)
	    Outputslayer = net.forward(ln)
	    #print(outputslayer
	    # detected bounding boxes, centroids, and confidences, respectively initilized
	    boxes = []
	    confidences = []
	    centroids = []
              #layer outputs iterated using for loop
	    for output in Outputslayer:
		# detections iterated using for loop
		for getdetection in output:
			#print(output,getdetection)
			#class ID and confidence is extracted  of the current object detection
			getscores = getdetection[5:]
			#print(getscores)
			classID = np.argmax(getscores)
			#print(classID)
			confidence = getscores[classID]
			#  detecting pixel distance is less then min conf
			if classID == personIdx and confidence > allowed_MIN_CONF:
				box = getdetection[0:4] * np.array([W, H, W, H])
				(centerX, centerY, width, height) = box.astype("int")
				#left corner of the bounding box and use the center (x, y)-coordinates to derive the top
				x = int(centerX - (width / 2))
				y = int(centerY - (height / 2))
				#print(x,y)
				#bounding box coordinates,centroids, and confidences is updated
				boxes.append([x, y, int(width), int(height)])
				centroids.append((centerX, centerY))
				confidences.append(float(confidence))
              # apply non-maxima suppression 
            idxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)
            # ensure at least one detection exists
            if len(idxs) > 0:
              # loop over the indexes we are keeping
              for i in idxs.flatten():
                # extract the bounding box coordinates
                (x, y) = (boxes[i][0], boxes[i][1])
                (w, h) = (boxes[i][2], boxes[i][3])
                r = boxes[i]
                # update our results list to consist of the person
                # prediction probability, bounding box coordinates,
                # and the centroid
                #r = (confidences[i], boxes[i],(x, y, w, h ,x + w, y + h), centroids[i])
                r = boxes[i] 
                results.append(r)
            # return the list of results
            return results
        results = detect_people(frame,net,ln1,personIdx=0)
        # Here we  bottom center points to bird eye view will be using bottom center point of bounding box for all boxes and will transform all those
        boxes1 = results
	#print(boxes1)
        person_points = utills.get_transformed_points(boxes1, prespective_transform)
        
        # Here we do  distance between transformed points(humans) will calculate distance between transformed points(humans)
        distances_mat, bxs_mat = utills.get_distances(boxes1, person_points, distance_w, distance_h)
	#print(distances_mat)
        risk_count = utills.get_count(distances_mat)
    
        frame1 = np.copy(frame)
        
        # according to risk factor Draw bird eye view and frame with bouding boxes around humans    
        bird_image = plot1.bird_eye_view(frame, distances_mat, person_points, scale_w, scale_h, risk_count)
        img = plot1.social_distancing_view(frame1, bxs_mat, boxes1, risk_count)
	#print(img.shape)
        
        # Show/write image and videos
        if count != 0:
            output_movie.write(img)
            bird_movie.write(bird_image)
    
            cv2.imshow('Bird Eye View', bird_image)
            cv2.imshow('Output video',img)
            cv2.imwrite(output_dir+"frame%d.jpg" % count, img)
            #cv2.imwrite(output_dir+"bird_eye_view/frame%d.jpg" % count, bird_image)
    
        count = count + 1
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
     
    vs.release()
    cv2.destroyAllWindows() 
        

if __name__== "__main__":

    # Receives arguements specified by user
    parser = argparse.ArgumentParser()
    
    parser.add_argument('-v', '--video_path', action='store', dest='video_path', default='./data/example.mp4' ,
                    help='Path for input video')
                    
    parser.add_argument('-o', '--output_dir', action='store', dest='output_dir', default='./output/' ,
                    help='Path for Output images')
    
    parser.add_argument('-O', '--output_vid', action='store', dest='output_vid', default='./output_vid/' ,
                    help='Path for Output videos')

    parser.add_argument('-m', '--model', action='store', dest='model', default='./models/',
                    help='Path for models directory')
                    
    parser.add_argument('-u', '--uop', action='store', dest='uop', default='NO',
                    help='Use open pose or not (YES/NO)')
                    
    values = parser.parse_args()
    
    model_path = values.model
    if model_path[len(model_path) - 1] != '/':
        model_path = model_path  
        
    output_dir = values.output_dir
    if output_dir[len(output_dir) - 1] != '/':
        output_dir = output_dir  
    
    output_vid = values.output_vid
    if output_vid[len(output_vid) - 1] != '/':
        output_vid = output_vid  


    # load Yolov3 weights
    
    weightsPath = model_path + "yolov3.weights"
    configPath = model_path + "yolov3.cfg"

    net_yl = cv2.dnn.readNetFromDarknet(configPath, weightsPath)
    ln = net_yl.getLayerNames()
    ln1 = [ln[i[0] - 1] for i in net_yl.getUnconnectedOutLayers()]

    # set mouse callback 

    cv2.namedWindow("image")
    cv2.setMouseCallback("image", get_mouse_points)
    np.random.seed(42)
    
    calculate_social_distancing(values.video_path, net_yl, output_dir, output_vid, ln1)